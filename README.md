# KnowledgeDistillation
Idea is to distill knowledge from large model to small model for faster inference and comparatively similar accuracy 
