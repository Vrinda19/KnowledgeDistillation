# KnowledgeDistillation
Idea is to distill knowledge from large model to small model for faster inference and comparatively similar accuracy 

## Dataset
MNIST 

## Model 
Teacher model: \
Student model: 

## Distillation Technique 

## Results 


----------------------------------------------
 ## Presentation contains 
 - What is knowledge distillation? \
 - Need for knowledge distillation \ 
 - Other SOTA techniques for knowledge distillation \
 - My experiment \
 - Future direction in this area 

